{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/home/dong/Documents/sonar/data/ebooks/Study Help/'\n",
    "names = os.listdir(base_dir)\n",
    "paths = [os.path.join(base_dir, name) for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_lines(lines):\n",
    "    filtered_lines = []\n",
    "\n",
    "    para = []\n",
    "    for i, line in enumerate(lines):\n",
    "        #print(i)\n",
    "        line = line.strip('\\x0c')#.strip('p.')\n",
    "        words = line.split(' ')\n",
    "        \n",
    "        if i > 1000 and words[0].strip('\\n').lower() in ['appendix', 'index', 'glossory', 'bibliography']:\n",
    "            #print(i, 'I am here')\n",
    "            break\n",
    "            \n",
    "        elif line == '\\n':\n",
    "            \n",
    "            # combine lines\n",
    "            para = \" \".join(p.rstrip() for p in para)\n",
    "            filtered_lines.append(para)\n",
    "            para = []\n",
    "\n",
    "        else:\n",
    "            \n",
    "            # remove certain words\n",
    "            new_words = []\n",
    "            bad_words = ['www', 'http', 'cid', 'ISBN']\n",
    "            for word in words:\n",
    "                word = word.strip(' ')\n",
    "#                 if len(word) > 0 and word[0] == ' business':\n",
    "#                     print(word)\n",
    "                if sum([b in word for b in bad_words]) == 0 and word != '':\n",
    "                    new_words.append(word)\n",
    "            #print(new_words)\n",
    "                    \n",
    "           # deal with — 1 — infront\n",
    "            three = new_words[:3]\n",
    "            if len(three) >= 3 and three[0] == '—' and three[2] == '—': # '—', '1', '—'\n",
    "                new_words = new_words[3:]\n",
    "                \n",
    "#           # remove the digit if a digit is in front of a para\n",
    "            condition1 = words[0].isdigit()\n",
    "            condition2 = len(words[0]) > 1 and words[0][0].isdigit()\n",
    "            condition3 = len(words[0]) == 2 and words[0][0].isalpha() and (words[0][-1] == '.' or word[0][-1] ==')')\n",
    "            condition4 = words[0] == 'p.' and len(words[1]) > 1 and words[1][0].isdigit() and (words[1][-1] == '.' or word[1][-1] ==')')\n",
    "            #print(condition4, words)\n",
    "            if condition1 or condition2 or condition3:\n",
    "                new_words = new_words[1:]\n",
    "            if condition4:\n",
    "                new_words = new_words[2:]\n",
    "                \n",
    "            ### what lines to include\n",
    "            condition1 = len(new_words) > 1 and new_words[0].lower() not in ['chapter', 'section']\n",
    "            condition2 = sum([len(word)==1 for word in new_words]) < int(0.5 * len(words)) # deal with 'i n t r o d u c t i o n'..............\n",
    "            condition3 = sum(['.....' in word for word in new_words]) < 1 # deal with .................\n",
    "            if condition1 and condition2 and condition3: # and condition5:\n",
    "                new_line = ' '.join([word for word in new_words])\n",
    "                para.append(new_line)\n",
    "\n",
    "\n",
    "        #skip the ending part\n",
    "\n",
    "    if len(filtered_lines) > 0:\n",
    "        count = 0\n",
    "        filtered_lines2 = []\n",
    "        para = [filtered_lines[0]]\n",
    "        for i, line in enumerate(filtered_lines[1:]):\n",
    "            words = line.split(' ')\n",
    "            if words[0].islower():\n",
    "                para.append(line)\n",
    "            else:\n",
    "                para = \" \".join(p for p in para)\n",
    "                filtered_lines2.append(para)\n",
    "                para = [line]\n",
    "            \n",
    "            \n",
    "        if len(filtered_lines2) > 0:\n",
    "            filtered_lines3 = []\n",
    "            for i, line in enumerate(filtered_lines2):\n",
    "                words = line.split(' ')\n",
    "                condition1 = (len(words) >=  5)\n",
    "                condition2 = (not words[-1].isdigit())\n",
    "                condition3 = sum([word.isupper() for word in words]) < min(len(words), 5)\n",
    "                condition4 = sum([word.isdigit() for word in words]) < min(len(words), 5)\n",
    "                condition5 = sum(['-' in word for word in words]) < min(len(words), 2)\n",
    "\n",
    "                if condition1 and condition2 and condition3 and condition4 and condition5:\n",
    "                    filtered_lines3.append(line)\n",
    "                    \n",
    "    try:\n",
    "        print(len(lines), len(filtered_lines), len(filtered_lines2), len(filtered_lines3))\n",
    "        return filtered_lines3\n",
    "    except NameError:\n",
    "        return filtered_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 /home/dong/Documents/sonar/data/ebooks/Study Help/TheMindMapBook.txt\n",
      "\n",
      "1 /home/dong/Documents/sonar/data/ebooks/Study Help/1905940629BoostYourMemory.txt\n",
      "6203 840 613 468\n",
      "\n",
      "2 /home/dong/Documents/sonar/data/ebooks/Study Help/Speed Reading.txt\n",
      "\n",
      "3 /home/dong/Documents/sonar/data/ebooks/Study Help/The Mindmap Book.txt\n",
      "\n",
      "4 /home/dong/Documents/sonar/data/ebooks/Study Help/The Ultimate Book of Mind Maps - Tony Buzan.txt\n",
      "5086 1421 1321 685\n",
      "\n",
      "5 /home/dong/Documents/sonar/data/ebooks/Study Help/How To Develop A Perfect Memory (Dominic O'brien) Quantum Memory Power.txt\n",
      "10003 2239 2021 852\n",
      "\n",
      "6 /home/dong/Documents/sonar/data/ebooks/Study Help/How To Develop A Super Power Memoryhow To Develop A Super-Power Memory - Harry Lorayne.txt\n",
      "11272 2648 1711 827\n",
      "\n",
      "7 /home/dong/Documents/sonar/data/ebooks/Study Help/How To Develop A Perfect Memory.txt\n",
      "\n",
      "8 /home/dong/Documents/sonar/data/ebooks/Study Help/Speed Memory.txt\n",
      "6944 1619 1522 651\n",
      "\n",
      "9 /home/dong/Documents/sonar/data/ebooks/Study Help/Alpha-netics_Speed Reading Course.txt\n",
      "5160 1683 1567 436\n",
      "\n",
      "10 /home/dong/Documents/sonar/data/ebooks/Study Help/How to Study by Ron Fry.txt\n",
      "10230 1312 1135 515\n",
      "\n",
      "11 /home/dong/Documents/sonar/data/ebooks/Study Help/Mind Mapping with FreeMind.txt\n",
      "3356 843 701 494\n",
      "\n",
      "12 /home/dong/Documents/sonar/data/ebooks/Study Help/College Success Guaranteed 5 Rules to Make it Happen.txt\n",
      "5170 1304 1217 419\n",
      "\n",
      "13 /home/dong/Documents/sonar/data/ebooks/Study Help/Protein Shakes for the Brain 91 Games and Exercises to Work Your Minds Muscle to the Max.txt\n",
      "6133 2203 1930 485\n",
      "\n",
      "14 /home/dong/Documents/sonar/data/ebooks/Study Help/TTC OPTIMIZING BRAIN FITNESS.PDF.txt\n",
      "3834 605 549 217\n",
      "\n",
      "15 /home/dong/Documents/sonar/data/ebooks/Study Help/Memory Improvement How To Improve Your Memory In Just 30 Days - Ron White - Mantesh.txt\n",
      "7610 2325 2245 390\n",
      "\n",
      "16 /home/dong/Documents/sonar/data/ebooks/Study Help/The Good Study Guide, 2ed - Andrew Northedge.txt\n",
      "18627 3496 3417 1537\n",
      "\n",
      "17 /home/dong/Documents/sonar/data/ebooks/Study Help/How to Develop a Brilliant Memory by Dominic O'Brien, 2014 Edition.txt\n",
      "5065 1196 1137 580\n",
      "\n",
      "18 /home/dong/Documents/sonar/data/ebooks/Study Help/Speed Reading Course - Peter Shepherd.txt\n",
      "1438 255 221 152\n",
      "\n",
      "19 /home/dong/Documents/sonar/data/ebooks/Study Help/The Buzan Study Skills Handbook bettr than use ur head.txt\n",
      "6662 1644 1341 583\n",
      "\n",
      "20 /home/dong/Documents/sonar/data/ebooks/Study Help/30 Days to a More Powerful Memory .txt\n",
      "12350 2608 2332 1193\n"
     ]
    }
   ],
   "source": [
    "for i, path in enumerate(paths): #[i:i+1]:\n",
    "    with open(path, 'r',  encoding=\"utf-8\") as f:\n",
    "        print()\n",
    "        print(i, path)\n",
    "        lines = f.readlines()\n",
    "        filtered_lines = cleanup_lines(lines)\n",
    "        \n",
    "\n",
    "\n",
    "    with open(os.path.join('Study Help.txt'), 'a',  encoding=\"utf-8\") as f:\n",
    "        for line in filtered_lines:\n",
    "            f.write(line+'\\n')\n",
    "            f.write('\\n')\n",
    "#             print(line)\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
